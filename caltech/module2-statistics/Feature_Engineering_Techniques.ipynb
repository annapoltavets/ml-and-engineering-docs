{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Common issues\n",
    "1. Missing data\n",
    "2. Categorical variables\n",
    "3. Imbalanced data\n",
    "4. Linear assumptions\n",
    "5. Outliers\n",
    "6. Feature scale\n",
    "7. High dimensionality (many features/variables)"
   ],
   "metadata": {
    "id": "aoKiYaho0eQ-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Imputing missing values\n",
    "\n",
    "### Numerical\n",
    "- Mean/Median imputation\n",
    "- Arbitrary\n",
    "- End of tail\n",
    "\n",
    "### Categorical\n",
    "- Mode\n",
    "- Add \"Missing\" category\n",
    "\n",
    "### Numerical and Categorical\n",
    "- Complete Case Analysis\n",
    "- Add a missing indicator\n",
    "- Random sample imputation"
   ],
   "metadata": {
    "id": "ETp9AlOO2Dnq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Before applying feature engineering, split yout dataset in train  and test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=.25,\n",
    "                                                    random_state=42)"
   ],
   "metadata": {
    "id": "t0v6E9Ms8N-R",
    "ExecuteTime": {
     "end_time": "2025-08-13T00:57:23.064151Z",
     "start_time": "2025-08-13T00:57:22.374859Z"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Before applying feature engineering, split yout dataset in train  and test\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m train_test_split\n\u001B[0;32m----> 5\u001B[0m X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m train_test_split(\u001B[43mX\u001B[49m,\n\u001B[1;32m      6\u001B[0m                                                     y,\n\u001B[1;32m      7\u001B[0m                                                     test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m.25\u001B[39m,\n\u001B[1;32m      8\u001B[0m                                                     random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'X' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3LTESUTCyi5v",
    "ExecuteTime": {
     "end_time": "2025-08-13T00:57:23.070030Z",
     "start_time": "2025-08-13T00:57:23.068331Z"
    }
   },
   "outputs": [],
   "source": [
    "# Numerical - Mean/Median Imputation\n",
    "\n",
    "#With sklearn\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# for normal distributions\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# for skewed distributions\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Fit and transform\n",
    "imputer.fit(X_train)\n",
    "X_train = imputer.transform(X_train)\n",
    "X_test = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%pip install feature-engine -q\n",
    "import pandas as pd\n",
    "from feature_engine.imputation import MeanMedianImputer\n",
    "\n",
    "\n",
    "# Load the Titanic dataset\n",
    "\n",
    "titanic_df = pd.read_csv('https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv')\n",
    "mmi = MeanMedianImputer(imputation_method='mean')\n",
    "mmi.fit(titanic_df)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98
    },
    "id": "0ZoiwIvgYAdh",
    "outputId": "ffb27f96-ef7b-4be2-ec79-f45f7acc6520",
    "ExecuteTime": {
     "end_time": "2025-08-13T00:57:23.071017Z",
     "start_time": "2025-08-13T00:57:23.070836Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "mmi.imputer_dict_"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c_7BaVY9YQPK",
    "outputId": "21bbf361-a20f-4ca4-f179-0002821dabcf",
    "ExecuteTime": {
     "end_time": "2025-08-13T00:57:23.078382Z",
     "start_time": "2025-08-13T00:57:23.073188Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "mmm = MeanMedianImputer(imputation_method='median')\n",
    "mmm.fit(titanic_df)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "_VOzgjtbYcXx",
    "outputId": "c1c289b7-9ad8-470c-9ff5-a0f415b85444",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.075942Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "mmm.imputer_dict_"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mRslZmCOYhMP",
    "outputId": "e3c7e622-82cb-42c3-a22f-a59e643222cc",
    "ExecuteTime": {
     "end_time": "2025-08-13T00:57:23.085761Z",
     "start_time": "2025-08-13T00:57:23.078930Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Numerical - Mean/Median Imputation\n",
    "\n",
    "#With feature_engine\n",
    "\n",
    "from feature_engine.imputation import MeanMedianImputer\n",
    "\n",
    "# for normal distributions\n",
    "\n",
    "mmi = MeanMedianImputer(imputation_method='mean')\n",
    "\n",
    "# for skewed distributions\n",
    "\n",
    "mmi = MeanMedianImputer(imputation_method='median')\n",
    "\n",
    "# Fit and transform\n",
    "\n",
    "mmi.fit(X_train)\n",
    "X_train = mmi.transform(X_train)\n",
    "X_test = mmi.transform(X_test)"
   ],
   "metadata": {
    "id": "uHLXtA21ltKF",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.081940Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "***Assumptions***\n",
    "- Data is missing at random\n",
    "- Missing data would look like most of your observations\n",
    "\n",
    "**Pros**\n",
    "- Easy to implement\n",
    "- Fast\n",
    "- Can be used in production\n",
    "\n",
    "**Cons**\n",
    "- Distorts: distributions, variance and covariance\n",
    "- The more missing values the higher the distortion"
   ],
   "metadata": {
    "id": "MpWkg_7p3ead"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Numerical - Arbitrary Imputation wth sklearn\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# for normal distributions\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=999)\n",
    "\n",
    "# Fit and transform\n",
    "imputer.fit(X_train)\n",
    "X_train = imputer.transform(X_train)\n",
    "X_test = imputer.transform(X_test)"
   ],
   "metadata": {
    "id": "H7F6K25r3blb",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.084813Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Numerical - Arbitrary Imputation wth feature engine\n",
    "\n",
    "from feature_engine.imputation import ArbitraryNumberImputer\n",
    "arbitrary_imputer = ArbitraryNumberImputer(\n",
    "    arbitrary_number=-999,\n",
    "    )\n",
    "\n",
    "# Fit and transform\n",
    "arbitrary_imputer.fit(X_train)\n",
    "X_train = arbitrary_imputer.transform(X_train)\n",
    "X_test = arbitrary_imputer.transform(X_test)"
   ],
   "metadata": {
    "id": "tIcwjWtueSae",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.088776Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "***Assumptions***\n",
    "- Data is not missing at random\n",
    "\n",
    "**Pros**\n",
    "- Easy to implement\n",
    "- Fast\n",
    "- Can be used in production\n",
    "- Captures the importance of a value being missing\n",
    "\n",
    "**Cons**\n",
    "- Distorts: distributions, variance and covariance\n",
    "- The more missing values the higher the distortion\n",
    "- It can mask or create outliers\n",
    "- Be careful not to use values that are too similar to mean/median"
   ],
   "metadata": {
    "id": "94R2gE9O0ca2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Numerical - End ot tail imputation\n",
    "\n",
    "from feature_engine.imputation import EndTailImputer\n",
    "\n",
    "# for normal distributions\n",
    "imputer = EndTailImputer(imputation_method='gaussian', tail='both')\n",
    "\n",
    "# for skewed distributions\n",
    "imputer = EndTailImputer(imputation_method='iqr', tail='both')\n",
    "\n",
    "# Fit and transform\n",
    "imputer.fit(X_train)\n",
    "X_train = imputer.transform(X_train)\n",
    "X_test = imputer.transform(X_test)"
   ],
   "metadata": {
    "id": "tTuXsjTP6Sci",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.092271Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "***Assumptions***\n",
    "- Data is not missing at random\n",
    "\n",
    "**Pros**\n",
    "- Easy to implement\n",
    "- Fast\n",
    "- Can be used in production\n",
    "\n",
    "**Cons**\n",
    "- Distorts: distributions, variance and covariance\n",
    "- The more missing values the higher the distortion"
   ],
   "metadata": {
    "id": "i4HV6Qs47uYM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Categorical - Frequency/Mode imputation\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Fit and transform\n",
    "imputer.fit(X_train)\n",
    "X_train = imputer.transform(X_train)\n",
    "X_test = imputer.transform(X_test)"
   ],
   "metadata": {
    "id": "G4muSUq472Xi",
    "ExecuteTime": {
     "end_time": "2025-08-13T00:57:23.099934Z",
     "start_time": "2025-08-13T00:57:23.095973Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "***Assumptions***\n",
    "- Data is missing at random\n",
    "- Missing observations most likely look like the majority\n",
    "\n",
    "**Pros**\n",
    "- Easy to implement\n",
    "- Fast\n",
    "- Can be used in production\n",
    "\n",
    "**Cons**\n",
    "- Distorts in the relation between the most frequent values and other variables\n",
    "- Bias/Overrepresentation of the mode if there are many missing values"
   ],
   "metadata": {
    "id": "9rKazhXA-Z0R"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Categorical - \"Missing\" new category\n",
    "\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=\"Missing\")\n",
    "\n",
    "# Fit and transform\n",
    "imputer.fit(X_train)\n",
    "X_train = imputer.transform(X_train)\n",
    "X_test = imputer.transform(X_test)"
   ],
   "metadata": {
    "id": "u-LNbuDH_S4s",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.099274Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Pros**\n",
    "- Easy to implement\n",
    "- Fast\n",
    "- Can be used in production\n",
    "- Capture the importance of missing data\n",
    "- There are not assumptions of data missing at random or not\n",
    "\n",
    "**Cons**\n",
    "- If the number of missing values is small you can end up with a rare category"
   ],
   "metadata": {
    "id": "lSQV9jFF_8PF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Numerical and Categorical - Complete Case Analysis\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df.dropna(inplace=True)"
   ],
   "metadata": {
    "id": "pLg0ieIr_6yO",
    "ExecuteTime": {
     "end_time": "2025-08-13T00:57:23.143792Z",
     "start_time": "2025-08-13T00:57:23.102184Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "***Assumptions***\n",
    "- Data is missing at random\n",
    "\n",
    "**Pros**\n",
    "- Easy to implement\n",
    "- No data manipulation required\n",
    "- Preserves distributions\n",
    "\n",
    "**Cons**\n",
    "- A lot of observations can be discarded if there is a significant amount of missing data\n",
    "- Can create a biased dataset when your CCA differ from the original data\n",
    "- Can't be used in production\n",
    "\n",
    "**When to use CCA:**\n",
    "- Data is completely missing at random\n",
    "- No more than 5% of observation will be discarded"
   ],
   "metadata": {
    "id": "Q4_R70oyBHYZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Numerical and Categorical - Missing Indicator\n",
    "\n",
    "from pandas.core.internals import concat\n",
    "from sklearn.impute import MissingIndicator\n",
    "\n",
    "indicator = MissingIndicator(features='missing-only')\n",
    "\n",
    "# Fit\n",
    "indicator.fit(X_train)\n",
    "\n",
    "# Print and get columns with missing indicator\n",
    "print(X_train.columns[indicator.features_])\n",
    "temp = indicator.transform(X_train)\n",
    "\n",
    "# Create columns for each new indicator\n",
    "indicator_columns =[column + \"_NA_IND\" for column in X_train.columns[indicator.features_]]\n",
    "indicator_df = pd.DataFrame(temp, columns=indicator_columns)\n",
    "\n",
    "# Concat columns with indicators and rest of training data\n",
    "X_train= = pd.concat([X_train.reset_index(), indicator_df], axis=1)\n",
    "\n",
    "# Same for Test\n",
    "temp_test = indicator.transform(X_test)\n",
    "test_indicator_df = pd.DataFrame(temp_test, columns=indicator_columns)\n",
    "\n",
    "X_test = pd.concat([X_test.reset_index(), test_indicator_df], axis=1)"
   ],
   "metadata": {
    "id": "GV9NXmZZB9gZ",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.105946Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "***Assumptions***\n",
    "- Data is NOT misssing at random\n",
    "- Missing data can be predicted\n",
    "\n",
    "**Pros**\n",
    "- Easy to implement\n",
    "- Can be integrated in production\n",
    "- Captures the importance of missing data\n",
    "\n",
    "**Cons**\n",
    "- Expands the feature set\n",
    "- The original variable still has to be imputed\n",
    "- Many missing indicators may be very highly correlated"
   ],
   "metadata": {
    "id": "I8JrfkhsDYcd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from scipy.sparse.construct import random\n",
    "# Numerical and Categorical - Random Sample Imputation\n",
    "\n",
    "from feature_engine.imputation import RandomSampleImputer\n",
    "\n",
    "imputer = RandomSampleImputer(random_state=42)\n",
    "\n",
    "# Fit and transform\n",
    "imputer.fit(X_train)\n",
    "X_train = imputer.transform(X_train)\n",
    "X_test = imputer.transform(X_test)"
   ],
   "metadata": {
    "id": "_ESCLT9OHJQG",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.110825Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "***Assumptions***\n",
    "- Data is misssing at random\n",
    "- MIssing values are replaced with other values within the same distribution of the original variable\n",
    "\n",
    "**Pros**\n",
    "- Easy to implement\n",
    "- Can be integrated in production\n",
    "- It preserves distributions\n",
    "\n",
    "**Cons**\n",
    "- Randomness\n",
    "- If there are many missing values the relationships between imputed variables and other variables may be affected\n",
    "- Memory allocation in production due to need to store both original and imputed datsets during imputation"
   ],
   "metadata": {
    "id": "_So1v7lIHZCw"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "zrzvwRsRJN7K",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.114959Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Categorical Variables\n",
    "\n",
    "### Classic techniques\n",
    "- One-hot encoding\n",
    "- Frequency encoding\n",
    "- Ordinal / Label encoding\n",
    "\n",
    "### Monotonic relationships\n",
    "- Ordered label encoding (regression)\n",
    "- Mean encoding (regression)\n",
    "- Weight of Evidence (binary classification)\n",
    "- Probability Ratio (binary classification)\n",
    "\n",
    "### Other techniques\n",
    "- Rare encoding\n",
    "- Binary encoding\n",
    "- Decision Tree encoding"
   ],
   "metadata": {
    "id": "lOvdMTCzJtIQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Other - Rare Label encoding (first encoder to apply)\n",
    "\n",
    "from feature_engine.encoding import RareLabelEncoder\n",
    "\n",
    "encoder = RareLabelEncoder()\n",
    "\n",
    "# Fit and transform\n",
    "encoder.fit(X_train)\n",
    "X_train = encoder.transform(X_train)\n",
    "X_test = encoder.transform(X_test)\n"
   ],
   "metadata": {
    "id": "eq5TVGvDcZWz",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.118372Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Classic techniques - One-hot encoder\n",
    "\n",
    "from feature_engine.encoding import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Fit and transform\n",
    "encoder.fit(X_train)\n",
    "X_train = encoder.transform(X_train)\n",
    "X_test = encoder.transform(X_test)"
   ],
   "metadata": {
    "id": "_5jxUz9eLI8r",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.123686Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Pros**\n",
    "- Doesn't assume distributions\n",
    "- Retains all categorical variable information\n",
    "- Works very with linear models\n",
    "\n",
    "**Cons**\n",
    "- Expands the feature space\n",
    "- Doesn't add any extra information while encoding\n",
    "- Add sparsity\n",
    "- Possible dummy variables may be identical"
   ],
   "metadata": {
    "id": "kNNkr9QvMpop"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Classic techniques - Frequency encoding\n",
    "\n",
    "from feature_engine.encoding import CountFrequencyEncoder\n",
    "\n",
    "encoder = CountFrequencyEncoder(encoding_method='frequency')\n",
    "\n",
    "# Fit and transform\n",
    "encoder.fit(X_train)\n",
    "X_train = encoder.transform(X_train)\n",
    "X_test = encoder.transform(X_test)"
   ],
   "metadata": {
    "id": "adipem_xNtvb",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.126984Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Pros**\n",
    "- Easy to implement\n",
    "- Feature space remains the same size\n",
    "- Work well with tree-based algorithms\n",
    "\n",
    "**Cons**\n",
    "- Limitations with linear model\n",
    "- Does not handle new categories\n",
    "- If 2 or more categories have the same count/frequency information can be lost"
   ],
   "metadata": {
    "id": "C-74KbFcOng1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Classic techniques - Label encoding\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform\n",
    "encoder.fit(X_train)\n",
    "X_train = encoder.transform(X_train)\n",
    "X_test = encoder.transform(X_test)"
   ],
   "metadata": {
    "id": "1J9w-mpePiye",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.130125Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Pros**\n",
    "- Easy to implement\n",
    "- Feature space remains the same size\n",
    "- Work well with tree-based algorithms\n",
    "\n",
    "**Cons**\n",
    "- Limitations with linear model\n",
    "- Does not handle new categories\n",
    "- Doesn't add any extra valuable information while encoding\n",
    "- Creates an ordered relatiosnships between the categories"
   ],
   "metadata": {
    "id": "NjjAoEziU-53"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Monotinic techniques - Ordered encoding\n",
    "\n",
    "from feature_engine.encoding import OrdinalEncoder\n",
    "\n",
    "encoder = OrdinalEncoder(encoding_method='ordered')\n",
    "\n",
    "# Fit and transform\n",
    "encoder.fit(X_train, y_train)\n",
    "X_train = encoder.transform(X_train)\n",
    "X_test = encoder.transform(X_test)"
   ],
   "metadata": {
    "id": "JTGWHBaUVd5L",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.133586Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Pros**\n",
    "- Easy to implement\n",
    "- Feature space remains the same size\n",
    "- Creates a monotinic relationship between with the target variable\n",
    "- Works very well for regression problems\n",
    "\n",
    "**Cons**\n",
    "- Can overfit models"
   ],
   "metadata": {
    "id": "xvQRH416XI5e"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Monotinic - Mean Encoder\n",
    "\n",
    "from feature_engine.encoding import MeanEncoder\n",
    "\n",
    "encoder = MeanEncoder()\n",
    "\n",
    "# Fit and transform\n",
    "encoder.fit(X_train, y_train)\n",
    "X_train = encoder.transform(X_train)\n",
    "X_test = encoder.transform(X_test)"
   ],
   "metadata": {
    "id": "At45ZUVnX4JO",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.136667Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Pros**\n",
    "- Easy to implement\n",
    "- Feature space remains the same size\n",
    "- Creates a monotinic relationship between with the target variable\n",
    "- Works very well for regression problems\n",
    "\n",
    "**Cons**\n",
    "- Can overfit models\n",
    "- If 2 or more categories have the same mean as the target information and relationships can decrease"
   ],
   "metadata": {
    "id": "cnOXi7lNYkTz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Monotinic - Weight of Evidence (binary classification only)\n",
    "\n",
    "from feature_engine.encoding import WoEEncoder\n",
    "\n",
    "encoder = WoEEncoder()\n",
    "\n",
    "# Fit and transform\n",
    "encoder.fit(X_train, y_train)\n",
    "X_train = encoder.transform(X_train)\n",
    "X_test = encoder.transform(X_test)"
   ],
   "metadata": {
    "id": "oQKYGFUZY9bQ",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.140277Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Pros**\n",
    "- Easy to implement\n",
    "- Feature space remains the same size\n",
    "- Creates a monotinic relationship between with the target variable\n",
    "- Orders the categories in a log scale\n",
    "- Works great for analysis as it is easy to compare the transformed variables to determine which one is a better predictor\n",
    "\n",
    "**Cons**\n",
    "- Can overfit models\n",
    "- Undefined when denominatror is 0"
   ],
   "metadata": {
    "id": "dcXXiRlcZqyz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Monotonic - Probability Ratios encoding (binary classification only)\n",
    "\n",
    "from feature_engine.encoding import PRatioEncoder\n",
    "\n",
    "encoder = PRatioEncoder()\n",
    "\n",
    "# Fit and transform\n",
    "encoder.fit(X_train, y_train)\n",
    "X_train = encoder.transform(X_train)\n",
    "X_test = encoder.transform(X_test)"
   ],
   "metadata": {
    "id": "lWyOEClAabZc",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.142611Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Pros**\n",
    "- Easy to implement\n",
    "- Feature space remains the same size\n",
    "- Creates a monotinic relationship between with the target variable\n",
    "- Works well with linear models (as every other monotinic approach)\n",
    "- Works great for analysis as it is easy to compare the transformed variables to determine which one is a better predictor\n",
    "\n",
    "**Cons**\n",
    "- Can overfit models\n",
    "- Undefined when denominatror is 0"
   ],
   "metadata": {
    "id": "GPQxS-zubjfo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Other - Binary encoder\n",
    "\n",
    "from category_encoders import BinaryEncoder\n",
    "\n",
    "encoder = BinaryEncoder()\n",
    "\n",
    "# Fit and transform\n",
    "encoder.fit(X_train, y_train)\n",
    "X_train = encoder.transform(X_train)\n",
    "X_test = encoder.transform(X_test)"
   ],
   "metadata": {
    "id": "Su7XJvWsb8zd",
    "ExecuteTime": {
     "end_time": "2025-08-13T00:57:23.201234Z",
     "start_time": "2025-08-13T00:57:23.144793Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Pros**\n",
    "- Easy to implement\n",
    "- Feature space remains ALMOST the same size\n",
    "\n",
    "**Cons**\n",
    "- Difficult to intepret\n",
    "- Potential loss of information during encoding"
   ],
   "metadata": {
    "id": "kHTzmOo8eOyt"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Linear assumptions (Transformations)\n",
    "\n",
    "- Logarithmic (right skewness)\n",
    "- Square root (right skewness)\n",
    "- Reciprocal (both)\n",
    "- Exponential (power) (left skewness)\n",
    "- Box-Cox\n",
    "- Yeo-Johnson"
   ],
   "metadata": {
    "id": "M8jMjj1If2mi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Logarithmic\n",
    "\n",
    "**Description**\n",
    "The logarithmic transformation is commonly used to stabilize variance when data exhibits right skewness. It reduces the range of data and helps make the distribution more symmetric.\n",
    "\n",
    "**Pros**\n",
    "\n",
    "*\tReduces right skewness effectively.\n",
    "*\tHelps in handling outliers by compressing the data range.\n",
    "*\tUseful in making multiplicative relationships additive.\n",
    "\n",
    "**Cons**\n",
    "\n",
    "*\tCannot be used with non-positive values (zeros or negatives).\n",
    "*\tMay not perform well if the data contains many zeros or very small positive values.\n",
    "\n",
    "**When to Use**\n",
    "\n",
    "*\tWhen the data is positively skewed and you need to stabilize the variance.\n",
    "*\tSuitable for data that follows a multiplicative process.\n"
   ],
   "metadata": {
    "id": "00L8fF8k19RG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from feature_engine.transformation import LogTransformer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = pd.DataFrame({'feature': [10, 20, 30, 40, 50]})\n",
    "\n",
    "# Apply Logarithmic Transformation\n",
    "transformer = LogTransformer(variables=['feature'])\n",
    "data_transformed = transformer.fit_transform(data)\n",
    "\n",
    "print(data_transformed)"
   ],
   "metadata": {
    "id": "4V07h-4B2WCZ",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.146941Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Square Root Transformation\n",
    "\n",
    "**Description**\n",
    "\n",
    "The square root transformation is another technique used to stabilize variance for right-skewed data. It is less aggressive than the logarithmic transformation.\n",
    "\n",
    "**Pros**\n",
    "\n",
    "* Reduces right skewness while preserving more of the dataâ€™s original structure.\n",
    "* Can handle zero values (but not negative values).\n",
    "\n",
    "**Cons**\n",
    "\n",
    "* Cannot be used with negative values.\n",
    "*\tLess effective than logarithmic transformation for extremely skewed data.\n",
    "\n",
    "**When to Use**\n",
    "\n",
    "* When data is moderately skewed to the right.\n",
    "*\tWhen logarithmic transformation is too strong but variance still needs to be stabilized."
   ],
   "metadata": {
    "id": "jFInLWum2h3P"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from feature_engine.transformation import PowerTransformer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = pd.DataFrame({'feature': [10, 20, 30, 40, 50]})\n",
    "\n",
    "# Apply Square Root Transformation\n",
    "transformer = PowerTransformer(variables=['feature'], exp=0.5)\n",
    "data_transformed = transformer.fit_transform(data)\n",
    "\n",
    "print(data_transformed)"
   ],
   "metadata": {
    "id": "7IUAz2W22zbW",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.149621Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reciprocal Transformation\n",
    "\n",
    "**Description**\n",
    "\n",
    "The reciprocal transformation is useful for both right and left-skewed data. It involves transforming the data to its reciprocal (1/x).\n",
    "\n",
    "**Pros**\n",
    "\n",
    "* Effective for both right and left skewness.\n",
    "*\tCan handle large ranges of data.\n",
    "\n",
    "**Cons**\n",
    "\n",
    "*\tCannot be used with zero or negative values.\n",
    "*\tHighly sensitive to very small values.\n",
    "\n",
    "**When to Use**\n",
    "\n",
    "*\tWhen data exhibits either right or left skewness.\n",
    "*\tUseful for data that spans several orders of magnitude."
   ],
   "metadata": {
    "id": "RdTArlSs3BJP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from feature_engine.transformation import ReciprocalTransformer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = pd.DataFrame({'feature': [10, 20, 30, 40, 50]})\n",
    "\n",
    "# Apply Reciprocal Transformation\n",
    "transformer = ReciprocalTransformer(variables=['feature'])\n",
    "data_transformed = transformer.fit_transform(data)\n",
    "\n",
    "print(data_transformed)"
   ],
   "metadata": {
    "id": "ZkMeHwMY3SpJ",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.152024Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "###Exponential (Power) Transformation\n",
    "\n",
    "**Description**\n",
    "\n",
    "The exponential transformation is typically used for left-skewed data. It involves raising the data to a specified power.\n",
    "\n",
    "**Pros**\n",
    "\n",
    "*\tEffective for reducing left skewness.\n",
    "*\tCan handle a wide range of transformations by adjusting the power.\n",
    "\n",
    "**Cons**\n",
    "\n",
    "*\tChoice of power requires careful consideration and may need experimentation.\n",
    "*\tCan increase skewness if the wrong power is chosen.\n",
    "\n",
    "**When to Use**\n",
    "\n",
    "*\tWhen data is negatively skewed (left-skewed).\n",
    "*\tWhen a more flexible transformation is needed.\n"
   ],
   "metadata": {
    "id": "ZNI1GW4c3Xuf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from feature_engine.transformation import PowerTransformer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = pd.DataFrame({'feature': [-5, -3, -1, 1, 3]})\n",
    "\n",
    "# Apply Exponential (Power) Transformation with power=2\n",
    "transformer = PowerTransformer(variables=['feature'], exp=2)\n",
    "data_transformed = transformer.fit_transform(data)\n",
    "\n",
    "print(data_transformed)"
   ],
   "metadata": {
    "id": "W9WeFcaH3j7M",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.154158Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Box-Cox Transformation\n",
    "\n",
    "**Description**\n",
    "\n",
    "The Box-Cox transformation is a family of power transformations parameterized by lambda. It is useful for stabilizing variance and making the data more normally distributed.\n",
    "\n",
    "**Pros**\n",
    "\n",
    "*\tCan handle positive data and find an optimal transformation parameter (lambda).\n",
    "*\tVersatile and widely used for various types of data.\n",
    "\n",
    "**Cons**\n",
    "\n",
    "*\tCannot be used with non-positive values.\n",
    "*\tRequires estimation of the lambda parameter, which can be computationally intensive.\n",
    "\n",
    "**When to Use**\n",
    "\n",
    "*\tWhen data needs to be normalized and variance stabilized.\n",
    "*\tSuitable for continuous, positive data."
   ],
   "metadata": {
    "id": "xd4rPd-U3pS8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from feature_engine.transformation import BoxCoxTransformer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = pd.DataFrame({'feature': [10, 20, 30, 40, 50]})\n",
    "\n",
    "# Apply Box-Cox Transformation\n",
    "transformer = BoxCoxTransformer(variables=['feature'])\n",
    "data_transformed = transformer.fit_transform(data)\n",
    "\n",
    "print(data_transformed)"
   ],
   "metadata": {
    "id": "e6EWMeZFevfh",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.156177Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Yeo-Johnson Transformation\n",
    "\n",
    "**Description**\n",
    "\n",
    "The Yeo-Johnson transformation is a modification of the Box-Cox transformation that can handle both positive and negative values.\n",
    "\n",
    "**Pros**\n",
    "\n",
    "* Can handle both positive and negative values.\n",
    "*\tFinds an optimal transformation parameter (lambda) automatically.\n",
    "\n",
    "**Cons**\n",
    "\n",
    "*\tRequires estimation of the lambda parameter.\n",
    "*\tComputationally more intensive than simpler transformations.\n",
    "\n",
    "**When to Use**\n",
    "\n",
    "*\tWhen data includes both positive and negative values and variance needs to be stabilized.\n",
    "*\tSuitable for normalizing a wide variety of data distributions.\n"
   ],
   "metadata": {
    "id": "-koUX3Dk5sWn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from feature_engine.transformation import YeoJohnsonTransformer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = pd.DataFrame({'feature': [-10, -5, 0, 5, 10]})\n",
    "\n",
    "# Apply Yeo-Johnson Transformation\n",
    "transformer = YeoJohnsonTransformer(variables=['feature'])\n",
    "data_transformed = transformer.fit_transform(data)\n",
    "\n",
    "print(data_transformed)"
   ],
   "metadata": {
    "id": "iw5mN9cjkIix",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.159821Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Feature scales\n",
    "\n",
    "- Mean normalization\n",
    "- Standardization\n",
    "- Robust Scaling\n",
    "- Min Max\n",
    "- Absolute max\n",
    "- Unit norm"
   ],
   "metadata": {
    "id": "x6MtMbumlh7e"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Standardization\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# Fit and transform\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ],
   "metadata": {
    "id": "VyyJwqSzn_rR",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.162873Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Robust Scaler\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "\n",
    "# Fit and transform\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ],
   "metadata": {
    "id": "r5cNh9saoTxf",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.164847Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#  MinMax Scaler\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 10))\n",
    "\n",
    "# Fit and transform\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ],
   "metadata": {
    "id": "pYEOTNQdpIst",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.166781Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#  Unit Norm Scaler\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "scaler = Normalizer()\n",
    "\n",
    "# Fit and transform\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ],
   "metadata": {
    "id": "vr-kQYqbpyPn",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.168737Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imbalanced data\n",
    "\n"
   ],
   "metadata": {
    "id": "eGEvujZDpwjk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE, ADASYN"
   ],
   "metadata": {
    "id": "pDAYF8U3k5Dl",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.171242Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create imbalanced dataset\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_classes=2, weights=[0.9, 0.1], random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ],
   "metadata": {
    "id": "AFV53-pBk5n2",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.174951Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_model(X_train, y_train, X_test, y_test):\n",
    "    model = LogisticRegression(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "id": "4ILU8k-tle-F",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.177601Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print('Original dataset:')\n",
    "evaluate_model(X_train, y_train, X_test, y_test)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U10yKifJmGTi",
    "outputId": "09a9c50c-3131-4f43-d8b0-547c7db92788",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.179851Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Oversampling using SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "smote.fit(X_train, y_train)\n",
    "X_train_smote, y_train_smote = smote.resample(X_train, y_train)\n",
    "\n",
    "print('SMOTE dataset:')\n",
    "evaluate_model(X_train_smote, y_train_smote, X_test, y_test)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "V-4oolYOmOon",
    "outputId": "e2ed1f46-3376-4e89-f824-ef27402e6c9a",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.182280Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Oversampling using ADASYN\n",
    "\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train, y_train)\n",
    "\n",
    "print('ADASYN dataset:')\n",
    "evaluate_model(X_train_adasyn, y_train_adasyn, X_test, y_test)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mPcxJcTCmbgR",
    "outputId": "f8cd416c-1c9b-46a9-8fe5-e86525e3a869",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.184603Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "! pip install feature_engine"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6gsUxJcmrCwE",
    "outputId": "a1df0fbe-f933-4225-f353-9b2094c812f7",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.187018Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "metadata": {
    "id": "g-IBIf5X7KwY",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.189226Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "BRUQ0Fsf7d4v",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.191710Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "7euvYqzF7lMj",
    "ExecuteTime": {
     "start_time": "2025-08-13T00:57:23.195429Z"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
