{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13c2bf8e",
   "metadata": {},
   "source": [
    "## Langchain Expression Language Basics\n",
    "\n",
    "-  LangChain Expression Language is that any two runnables can be \"chained\" together into sequences. \n",
    "- The output of the previous runnable's .invoke() call is passed as input to the next runnable.\n",
    "- This can be done using the pipe operator (|), or the more explicit .pipe() method, which does the same thing.\n",
    "\n",
    "- Type of LCEL Chains\n",
    "    - SequentialChain\n",
    "    - Parallel Chain\n",
    "    - Router Chain\n",
    "    - Chain Runnables\n",
    "    - Custom Chain (Runnable Sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987ac1ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9bdb738f",
   "metadata": {},
   "source": [
    "1. What is LCEL?\n",
    "\n",
    "LCEL stands for LangChain Expression Language.\n",
    "It provides a concise, composable, and Pythonic way to build chains, pipelines, and workflows in LangChain.\n",
    "\n",
    "Goal: Replace the older SequentialChain, RouterChain, etc., with a more flexible and declarative API.\n",
    "Key Concepts: Chains as functions (or callables), easy composition (| operator), clear input/output typing.\n",
    "\n",
    "2. Core LCEL Components\n",
    "\n",
    "a. Runnable\n",
    "The core abstraction in LCEL.\n",
    "Any object that implements the .invoke() method and can be composed using the | operator.\n",
    "Examples: Models, PromptTemplates, Chains.\n",
    "\n",
    "b. PromptTemplate\n",
    "Used to format inputs into prompts for LLMs or ChatModels.\n",
    "LCEL expects prompt templates to have variable names matching the input keys.\n",
    "\n",
    "c. LLM and ChatModel\n",
    "Language Models and ChatModels from langchain_openai, langchain_community, etc.\n",
    "Compatible with LCEL if they implement .invoke()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caaf3727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c9f5081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x10f72b390>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x10f72ffd0>, root_client=<openai.OpenAI object at 0x10f1b5410>, root_async_client=<openai.AsyncOpenAI object at 0x10f72b550>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import (\n",
    "                                        SystemMessagePromptTemplate,\n",
    "                                        HumanMessagePromptTemplate,\n",
    "                                        ChatPromptTemplate\n",
    "                                        )\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ea34732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The solar system consists of the Sun and all celestial bodies that orbit it, including planets, moons, asteroids, and comets.  \n",
      "2. There are eight major planets: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.  \n",
      "3. The four inner planets (Mercury, Venus, Earth, Mars) are rocky, while the outer planets (Jupiter, Saturn, Uranus, Neptune) are gas giants or ice giants.  \n",
      "4. The asteroid belt lies between Mars and Jupiter, containing many small rocky bodies.  \n",
      "5. The solar system formed about 4.6 billion years ago from a giant cloud of gas and dust.\n"
     ]
    }
   ],
   "source": [
    "system = SystemMessagePromptTemplate.from_template('You are {school} teacher. You answer in short sentences.')\n",
    "\n",
    "question = HumanMessagePromptTemplate.from_template('tell me about the {topics} in {points} points')\n",
    "\n",
    "\n",
    "messages = [system, question]\n",
    "template = ChatPromptTemplate(messages)\n",
    "\n",
    "question = template.invoke({'school': 'primary', 'topics': 'solar system', 'points': 5})\n",
    "\n",
    "response = llm.invoke(question)\n",
    "print(response.content)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4af29dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = SystemMessagePromptTemplate.from_template('You are {school} teacher. You answer in short sentences.')\n",
    "\n",
    "question = HumanMessagePromptTemplate.from_template('tell me about the {topics} in {points} points')\n",
    "\n",
    "\n",
    "messages = [system, question]\n",
    "template = ChatPromptTemplate(messages)\n",
    "\n",
    "chain = template | llm\n",
    "#LLMChain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd5d25a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The solar system consists of the Sun and all celestial bodies that orbit it, including planets, moons, asteroids, and comets.  \n",
      "2. There are eight major planets: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.  \n",
      "3. The four inner planets (Mercury, Venus, Earth, Mars) are rocky, while the four outer planets (Jupiter, Saturn, Uranus, Neptune) are gas giants.  \n",
      "4. The asteroid belt lies between Mars and Jupiter, containing many small rocky bodies.  \n",
      "5. The solar system formed about 4.6 billion years ago from a giant cloud of gas and dust.  \n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({'school': 'primary', 'topics': 'solar system', 'points': 5})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcf78c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The solar system consists of the Sun and all celestial bodies that orbit it, including eight planets, moons, asteroids, and comets.  \n",
      "2. The eight planets are divided into two categories: terrestrial (Mercury, Venus, Earth, Mars) and gas giants (Jupiter, Saturn) and ice giants (Uranus, Neptune).  \n",
      "3. The Sun accounts for about 99.86% of the solar system's total mass, providing the necessary heat and light for life on Earth.  \n",
      "4. The asteroid belt lies between Mars and Jupiter, containing numerous rocky bodies, while the Kuiper Belt and Oort Cloud are regions of icy objects beyond Neptune.  \n",
      "5. The solar system formed approximately 4.6 billion years ago from a rotating disk of gas and dust, known as the solar nebula.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({'school': 'phd', 'topics': 'solar system', 'points': 5})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22ed93c",
   "metadata": {},
   "source": [
    "# What is StrOutputParser?\n",
    "\n",
    "\n",
    "StrOutputParser is a utility class in LangChain for handling the output of language models (LLMs/ChatModels).\n",
    "It parses the model’s response and returns it as a plain Python string, removing any unnecessary metadata, wrappers, or objects that the model or API may return.\n",
    "\n",
    "\n",
    "## Why do we need it?\n",
    "\n",
    "- By default, LLMs or chat models in LangChain might return structured objects, message objects, or dictionaries.\n",
    "- In most cases, especially for simple chains, you just want the generated text string (e.g., the answer, summary, completion, etc.).\n",
    "- StrOutputParser extracts this text from the raw output so your chain returns a clean string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e95d027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without StrParser\n",
      "\n",
      "content='The capital of India is New Delhi.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 19, 'total_tokens': 27, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_8bda4d3a2c', 'id': 'chatcmpl-CEQ6z4nUlrkeaItqEck7LjPoVrpnr', 'finish_reason': 'stop', 'logprobs': None} id='run-e132f30b-8b4e-4f6e-b9ca-951d1fc90f35-0' usage_metadata={'input_tokens': 19, 'output_tokens': 8, 'total_tokens': 27, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "With StrParser\n",
      "\n",
      "The capital of India is New Delhi.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What is the capital of {country}?\")\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.5)\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain1 = prompt | llm \n",
    "chain2 = prompt | llm | parser\n",
    "\n",
    "print(\"Without StrParser\\n\")\n",
    "resultChain1 = chain1.invoke({\"country\" , \"India\"})\n",
    "print(resultChain1)\n",
    "\n",
    "print(\"\\nWith StrParser\\n\")\n",
    "resultChain2 = chain2.invoke({\"country\": \"India\"})\n",
    "print(resultChain2)  # Output: \"The capital of India is New Delhi.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d16fdc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The solar system consists of the Sun and eight planets.  \n",
      "2. The planets are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.  \n",
      "3. It also includes dwarf planets, moons, asteroids, and comets.  \n",
      "4. The Sun is a star and provides light and heat.  \n",
      "5. The solar system formed about 4.6 billion years ago.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "#template | model | parser(optional)\n",
    "\n",
    "chain = template | llm | StrOutputParser()\n",
    "response = chain.invoke({'school': 'primary', 'topics': 'solar system', 'points': 5})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cda9ec44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.runnables.base.RunnableSequence"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93db13a1",
   "metadata": {},
   "source": [
    "### Chaining Runnables (Chain Multiple Runnables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c1b07c",
   "metadata": {},
   "source": [
    "- We can even combine this chain with more runnables to create another chain.\n",
    "- Let's see how easy our generated output is?\n",
    "\n",
    "\n",
    "# Concept: What Are Runnables and Chaining?\n",
    "\n",
    "Runnables in LangChain are modular, composable components—like prompts, LLMs, output parsers, or even custom functions—that implement an .invoke() method.\n",
    "\n",
    "Chaining Runnables means connecting these modular components in sequence, so the output of one becomes the input of the next. In LCEL, you use the pipe operator (|) to compose them functionally, much like Unix pipes or Pandas method chaining.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b8368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecbf745a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text is relatively easy to understand, as it presents straightforward information about the solar system in a clear and concise manner.\n"
     ]
    }
   ],
   "source": [
    "analysis_prompt = ChatPromptTemplate.from_template('''analyze the following text: {response}\n",
    "                                                   You need tell me that how difficult it is to understand.\n",
    "                                                   Answer in one sentence only.\n",
    "                                                   ''')\n",
    "\n",
    "fact_check_chain = analysis_prompt | llm | StrOutputParser()\n",
    "output = fact_check_chain.invoke({'response': response})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d028739f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text is moderately easy to understand for someone with a basic knowledge of astronomy, as it presents clear information about the solar system and its components without overly complex terminology.\n"
     ]
    }
   ],
   "source": [
    "#SequentialChain\n",
    "\n",
    "# chain1 ---> chain2\n",
    "composed_chain = {\"response\": chain} | analysis_prompt | llm | StrOutputParser()\n",
    "\n",
    "output = composed_chain.invoke({'school': 'phd', 'topics': 'solar system', 'points': 5})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b4800d",
   "metadata": {},
   "source": [
    "### Parallel LCEL Chain\n",
    "- Parallel chains are used to run multiple runnables in parallel.\n",
    "- The final return value is a dict with the results of each value under its appropriate key.\n",
    "\n",
    "\n",
    "### What is a Parallel Chain?\n",
    "\n",
    "Parallel chaining lets you run multiple chains (pipelines) simultaneously using the same or similar inputs, collecting all outputs together.\n",
    "\n",
    "In LangChain LCEL, this is achieved via RunnableParallel.\n",
    "\n",
    "It’s analogous to a “fan-out” pattern, where you branch out your input to multiple tasks and gather all their results in one go.\n",
    "\n",
    "### Why Use Parallel Chains?\n",
    "\n",
    "- To answer multiple questions about the same input in one shot.\n",
    "- To generate diverse content (e.g., facts, poems, summaries) from the same base information.\n",
    "- To save code and avoid running chains sequentially when there’s no dependency between them.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7bd2f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The solar system consists of the Sun and eight planets, including Earth, along with moons, asteroids, and comets.  \n",
      "2. Planets are divided into inner (terrestrial) and outer (gas giants) groups based on their composition and distance from the Sun.\n"
     ]
    }
   ],
   "source": [
    "system = SystemMessagePromptTemplate.from_template('You are {school} teacher. You answer in short sentences.')\n",
    "\n",
    "question = HumanMessagePromptTemplate.from_template('tell me about the {topics} in {points} points')\n",
    "\n",
    "\n",
    "messages = [system, question]\n",
    "template = ChatPromptTemplate(messages)\n",
    "fact_chain = template | llm | StrOutputParser()\n",
    "\n",
    "output = fact_chain.invoke({'school': 'primary', 'topics': 'solar system', 'points': 2})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90ab5616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planets spin in cosmic dance,  \n",
      "Stars above in vast expanse.\n"
     ]
    }
   ],
   "source": [
    "question = HumanMessagePromptTemplate.from_template('write a poem on {topics} in {sentences} lines')\n",
    "\n",
    "\n",
    "messages = [system, question]\n",
    "template = ChatPromptTemplate(messages)\n",
    "poem_chain = template | llm | StrOutputParser()\n",
    "\n",
    "output = poem_chain.invoke({'school': 'primary', 'topics': 'solar system', 'sentences': 2})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479bf9a5",
   "metadata": {},
   "source": [
    "What happens here?\n",
    "\n",
    "- The same input dictionary is sent to both fact_chain and poem_chain.\n",
    "- Both chains execute independently (in parallel in logic, not necessarily true multi-threaded parallelism).\n",
    "- Output is a dictionary:\n",
    "\n",
    "{'fact': <fact_output>, 'poem': <poem_output>}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9842e0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The solar system consists of the Sun, eight planets, their moons, and other celestial bodies like asteroids and comets.  \n",
      "2. The four inner planets are rocky, while the four outer planets are gas giants.\n",
      "\n",
      "\n",
      "\n",
      "Planets orbit in a cosmic dance,  \n",
      "Stars twinkle bright, in the vast expanse.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "chain = RunnableParallel(fact = fact_chain, poem = poem_chain)\n",
    "\n",
    "\n",
    "output = chain.invoke({'school': 'primary', 'topics': 'solar system', 'points': 2, 'sentences': 2})\n",
    "print(output['fact'])\n",
    "print('\\n\\n')\n",
    "print(output['poem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab0384b",
   "metadata": {},
   "source": [
    "### Chain Router\n",
    "- The router chain is used to route the output of a previous runnable to the next runnable based on the output of the previous runnable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c143a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"Given the user review below, classify it as either being about `Positive` or `Negative`.\n",
    "            Do not respond with more than one word.\n",
    "\n",
    "            Review: {review}\n",
    "            Classification:\"\"\"\n",
    "\n",
    "template = ChatPromptTemplate.from_template(prompt)\n",
    "\n",
    "chain = template | llm | StrOutputParser()\n",
    "\n",
    "#review = \"Thank you so much for providing such a great plateform for learning. I am really happy with the service.\"\n",
    "review = \"I am not happy with the service. It is not good.\"\n",
    "chain.invoke({'review': review})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a960ee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_prompt = \"\"\"\n",
    "                You are expert in writing reply for positive reviews.\n",
    "                You need to encourage the user to share their experience on social media.\n",
    "                Review: {review}\n",
    "                Answer:\"\"\"\n",
    "\n",
    "positive_template = ChatPromptTemplate.from_template(positive_prompt)\n",
    "positive_chain = positive_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f49fc8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_prompt = \"\"\"\n",
    "                You are expert in writing reply for negative reviews.\n",
    "                You need first to apologize for the inconvenience caused to the user.\n",
    "                You need to encourage the user to share their concern on following Email:'pn@aadmin.com'.\n",
    "                Review: {review}\n",
    "                Answer:\"\"\"\n",
    "\n",
    "\n",
    "negative_template = ChatPromptTemplate.from_template(negative_prompt)\n",
    "negative_chain = negative_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b0b76b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rout(info):\n",
    "    if 'positive' in info['sentiment'].lower():\n",
    "        return positive_chain\n",
    "    else:\n",
    "        return negative_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16030065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  sentiment: ChatPromptTemplate(input_variables=['review'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['review'], input_types={}, partial_variables={}, template='Given the user review below, classify it as either being about `Positive` or `Negative`.\\n            Do not respond with more than one word.\\n\\n            Review: {review}\\n            Classification:'), additional_kwargs={})])\n",
       "             | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x121005090>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x1210062d0>, root_client=<openai.OpenAI object at 0x120d8f410>, root_async_client=<openai.AsyncOpenAI object at 0x120df3b90>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "             | StrOutputParser(),\n",
       "  review: RunnableLambda(lambda x: x['review'])\n",
       "}\n",
       "| RunnableLambda(rout)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "full_chain = {\"sentiment\": chain, 'review': lambda x: x['review']} | RunnableLambda(rout)\n",
    "\n",
    "full_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db480de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear [User's Name],\n",
      "\n",
      "Thank you for your kind words! We’re thrilled to hear that you’re enjoying our platform and finding it helpful for your learning journey. Your satisfaction means the world to us!\n",
      "\n",
      "If you feel inspired, we’d love for you to share your experience on social media. Your feedback can help others discover the benefits of our service. Don’t forget to tag us!\n",
      "\n",
      "Thank you once again for your support!\n",
      "\n",
      "Best regards,  \n",
      "[Your Name]  \n",
      "[Your Position]\n"
     ]
    }
   ],
   "source": [
    "review = \"Thank you so much for providing such a great platform for learning. I am really happy with the service.\"\n",
    "#review = \"I am not happy with the service. It is not good.\"\n",
    "\n",
    "output = full_chain.invoke({'review': review})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f99acd",
   "metadata": {},
   "source": [
    "### Make Custom Chain Runnables with RunnablePassthrough and RunnableLambda\n",
    "- This is useful for formatting or when you need functionality not provided by other LangChain components, and custom functions used as Runnables are called RunnableLambdas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3bf1364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input1', 'input2'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input1', 'input2'], input_types={}, partial_variables={}, template='Explain these inputs in 5 sentences: {input1} and {input2}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "def char_counts(text):\n",
    "    return len(text)\n",
    "\n",
    "def word_counts(text):\n",
    "    return len(text.split())\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Explain these inputs in 5 sentences: {input1} and {input2}\")\n",
    "\n",
    "prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8594e700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earth is a terrestrial planet that orbits the Sun, located in the third position from the Sun in our solar system. It has a diverse environment with land, water, and atmosphere, supporting a wide variety of life forms. The Sun, on the other hand, is a massive star composed mainly of hydrogen and helium, generating energy through nuclear fusion in its core. This energy radiates outwards, providing the heat and light necessary for life on Earth. Together, the Earth and the Sun are part of a complex gravitational dance that influences the behaviors and climates of the planet.\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "output = chain.invoke({'input1': 'Earth is planet', 'input2': 'Sun is star'})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10aaae79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'char_counts': 682, 'word_counts': 121, 'output': 'Earth is a terrestrial planet located in the Solar System, orbiting the Sun at an average distance of about 93 million miles. It is the third planet from the Sun and is known for its diverse ecosystems, atmosphere, and the presence of liquid water, which supports life. The Sun, on the other hand, is a massive ball of gas, primarily composed of hydrogen and helium, and is classified as a medium-sized star in the Milky Way galaxy. It serves as the central gravitational anchor of the Solar System, providing light and heat necessary for life on Earth. Together, the Earth and the Sun play crucial roles in the dynamics of the Solar System and the sustenance of life on our planet.'}\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm | StrOutputParser() | {'char_counts': RunnableLambda(char_counts), \n",
    "                                            'word_counts': RunnableLambda(word_counts), \n",
    "                                            'output': RunnablePassthrough()}\n",
    "\n",
    "output = chain.invoke({'input1': 'Earth is planet', 'input2': 'Sun is star'})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645bf4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3393ef6d",
   "metadata": {},
   "source": [
    "### Custom Chain using `@chain` decorator\n",
    "\n",
    "Custom Chains with @chain Decorator in LangChain (LCEL, v0.3.x)\n",
    "\n",
    "1. What is the `@chain` Decorator?\n",
    "\n",
    "- The @chain decorator is a feature of LangChain’s LCEL API (Expression Language).\n",
    "- It transforms any standard Python function into a Runnable Chain.\n",
    "- This allows you to wrap arbitrary Python logic—including loops, conditionals, and composition of other chains—into a component that behaves just like any other chain in LCEL (i.e., it supports .invoke(), .batch(), etc.).\n",
    "\n",
    "2. Why Use `@chain`?\n",
    "\n",
    "For complex, custom workflows that can’t be easily expressed by chaining with the | operator.\n",
    "To combine multiple sub-chains, add custom logic, or perform steps that need Python code (like aggregation, formatting, conditional logic).\n",
    "It enables full flexibility within the LCEL framework: your custom function becomes a first-class Runnable.\n",
    "\n",
    "3. Example Explained\n",
    "\n",
    "\n",
    "4. How Does it Work?\n",
    "\n",
    "The decorator wraps your function into a Runnable-compatible object.\n",
    "Supports all LCEL operations (invoke, batch, etc.).\n",
    "You can include arbitrary Python code—loops, conditionals, error handling, etc.\n",
    "The function input is a single parameter (often a dict).\n",
    "The output can be any object—dict, list, string, etc.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf85db67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import chain\n",
    "\n",
    "@chain\n",
    "def custom_chain(params):\n",
    "    return {\n",
    "        'fact': fact_chain.invoke(params),\n",
    "        'poem': poem_chain.invoke(params),\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645a7580",
   "metadata": {},
   "source": [
    "\n",
    "4. How Does it Work?\n",
    "\n",
    "- The decorator wraps your function into a Runnable-compatible object.\n",
    "- Supports all LCEL operations (invoke, batch, etc.).\n",
    "- You can include arbitrary Python code—loops, conditionals, error handling, etc.\n",
    "- The function input is a single parameter (often a dict).\n",
    "- The output can be any object—dict, list, string, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055a1805",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = {'school': 'primary', 'topics': 'solar system', 'points': 2, 'sentences': 2}\n",
    "output = custom_chain.invoke(params)\n",
    "print(output['fact'])\n",
    "print('\\n\\n')\n",
    "print(output['poem'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
