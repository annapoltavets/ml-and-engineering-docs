{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#  HR Assistant",
   "id": "d6ea3a49e6eb46cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Action\n",
    "\n",
    "This project implements an HR Assistant chatbot for Nestlé using RAG (Retrieval-Augmented Generation) architecture. Here's what we'll accomplish:\n",
    "\n",
    "### 1. **Environment Setup**\n",
    "- Import essential tools and set up OpenAI's API environment\n",
    "- Load environment variables for secure API access\n",
    "\n",
    "### 2. **Document Processing**\n",
    "- Load Nestlé's HR policy using PyPDFLoader\n",
    "- Split the document into manageable chunks for efficient processing\n",
    "- Use RecursiveCharacterTextSplitter with optimal chunk size and overlap\n",
    "\n",
    "### 3. **Vector Database Creation**\n",
    "- Create vector representations for text chunks using FAISS and OpenAI's embeddings\n",
    "- Build a searchable knowledge base for quick document retrieval\n",
    "- Save and load the vector database for future use\n",
    "\n",
    "### 4. **RAG System Implementation**\n",
    "- Build a question-answering system using GPT-4o-mini model\n",
    "- Implement retrieval chain to find relevant document chunks\n",
    "- Create a prompt template to guide the chatbot's responses\n",
    "\n",
    "### 5. **User Interface**\n",
    "- Use Gradio to build a user-friendly chatbot interface\n",
    "- Enable real-time interaction and information retrieval\n",
    "- Provide example questions and source citations"
   ],
   "id": "d53370489df51101"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T22:08:18.902933Z",
     "start_time": "2025-11-25T22:08:18.896675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "id": "c21612893d1b002c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T22:08:20.469343Z",
     "start_time": "2025-11-25T22:08:20.465293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from typing import List\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "\n",
    "# LCEL building blocks\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# UI\n",
    "import gradio as gr"
   ],
   "id": "e22017b41e1a39bb",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T22:08:21.093180Z",
     "start_time": "2025-11-25T22:08:21.090057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PDF_PATH = \"Dataset/the_nestle_hr_policy_pdf_2012.pdf\"\n",
    "CHAT_MODEL = \"gpt-4o-mini\""
   ],
   "id": "edfbbe426a79b063",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T22:08:22.174302Z",
     "start_time": "2025-11-25T22:08:21.858479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loader = PyPDFLoader(PDF_PATH)\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=50, separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "print(f\"Created {len(chunks)} chunks.\")"
   ],
   "id": "123a83264d9f7cfa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 35 chunks.\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T22:08:23.113113Z",
     "start_time": "2025-11-25T22:08:22.574777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "vectordb = FAISS.from_documents(chunks, embeddings)\n",
    "vectordb.save_local(\"index\")"
   ],
   "id": "b270aa0a393bd2c8",
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************JZsA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}}",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAuthenticationError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[21]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m embeddings = OpenAIEmbeddings()\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m vectordb = \u001B[43mFAISS\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_documents\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchunks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membeddings\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      4\u001B[39m vectordb.save_local(\u001B[33m\"\u001B[39m\u001B[33mindex\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anna-apps/ml-and-engineering-docs/.venv/lib/python3.11/site-packages/langchain_core/vectorstores/base.py:837\u001B[39m, in \u001B[36mVectorStore.from_documents\u001B[39m\u001B[34m(cls, documents, embedding, **kwargs)\u001B[39m\n\u001B[32m    834\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28many\u001B[39m(ids):\n\u001B[32m    835\u001B[39m         kwargs[\u001B[33m\"\u001B[39m\u001B[33mids\u001B[39m\u001B[33m\"\u001B[39m] = ids\n\u001B[32m--> \u001B[39m\u001B[32m837\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfrom_texts\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membedding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadatas\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmetadatas\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anna-apps/ml-and-engineering-docs/.venv/lib/python3.11/site-packages/langchain_community/vectorstores/faiss.py:1043\u001B[39m, in \u001B[36mFAISS.from_texts\u001B[39m\u001B[34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001B[39m\n\u001B[32m   1016\u001B[39m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[32m   1017\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mfrom_texts\u001B[39m(\n\u001B[32m   1018\u001B[39m     \u001B[38;5;28mcls\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1023\u001B[39m     **kwargs: Any,\n\u001B[32m   1024\u001B[39m ) -> FAISS:\n\u001B[32m   1025\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Construct FAISS wrapper from raw documents.\u001B[39;00m\n\u001B[32m   1026\u001B[39m \n\u001B[32m   1027\u001B[39m \u001B[33;03m    This is a user friendly interface that:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1041\u001B[39m \u001B[33;03m            faiss = FAISS.from_texts(texts, embeddings)\u001B[39;00m\n\u001B[32m   1042\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1043\u001B[39m     embeddings = \u001B[43membedding\u001B[49m\u001B[43m.\u001B[49m\u001B[43membed_documents\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1044\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m.__from(\n\u001B[32m   1045\u001B[39m         texts,\n\u001B[32m   1046\u001B[39m         embeddings,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1050\u001B[39m         **kwargs,\n\u001B[32m   1051\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anna-apps/ml-and-engineering-docs/.venv/lib/python3.11/site-packages/langchain_openai/embeddings/base.py:588\u001B[39m, in \u001B[36mOpenAIEmbeddings.embed_documents\u001B[39m\u001B[34m(self, texts, chunk_size)\u001B[39m\n\u001B[32m    585\u001B[39m \u001B[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001B[39;00m\n\u001B[32m    586\u001B[39m \u001B[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001B[39;00m\n\u001B[32m    587\u001B[39m engine = cast(\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mself\u001B[39m.deployment)\n\u001B[32m--> \u001B[39m\u001B[32m588\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_get_len_safe_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mengine\u001B[49m\u001B[43m=\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anna-apps/ml-and-engineering-docs/.venv/lib/python3.11/site-packages/langchain_openai/embeddings/base.py:483\u001B[39m, in \u001B[36mOpenAIEmbeddings._get_len_safe_embeddings\u001B[39m\u001B[34m(self, texts, engine, chunk_size)\u001B[39m\n\u001B[32m    481\u001B[39m batched_embeddings: List[List[\u001B[38;5;28mfloat\u001B[39m]] = []\n\u001B[32m    482\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m _iter:\n\u001B[32m--> \u001B[39m\u001B[32m483\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    484\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m=\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[43m_chunk_size\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_invocation_params\u001B[49m\n\u001B[32m    485\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    486\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, \u001B[38;5;28mdict\u001B[39m):\n\u001B[32m    487\u001B[39m         response = response.model_dump()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anna-apps/ml-and-engineering-docs/.venv/lib/python3.11/site-packages/openai/resources/embeddings.py:128\u001B[39m, in \u001B[36mEmbeddings.create\u001B[39m\u001B[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001B[39m\n\u001B[32m    122\u001B[39m             embedding.embedding = np.frombuffer(  \u001B[38;5;66;03m# type: ignore[no-untyped-call]\u001B[39;00m\n\u001B[32m    123\u001B[39m                 base64.b64decode(data), dtype=\u001B[33m\"\u001B[39m\u001B[33mfloat32\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    124\u001B[39m             ).tolist()\n\u001B[32m    126\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\n\u001B[32m--> \u001B[39m\u001B[32m128\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    129\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m/embeddings\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    130\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membedding_create_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43mEmbeddingCreateParams\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    131\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    132\u001B[39m \u001B[43m        \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    133\u001B[39m \u001B[43m        \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    134\u001B[39m \u001B[43m        \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    135\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    136\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpost_parser\u001B[49m\u001B[43m=\u001B[49m\u001B[43mparser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    137\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    138\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m=\u001B[49m\u001B[43mCreateEmbeddingResponse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    139\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anna-apps/ml-and-engineering-docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1242\u001B[39m, in \u001B[36mSyncAPIClient.post\u001B[39m\u001B[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[39m\n\u001B[32m   1228\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpost\u001B[39m(\n\u001B[32m   1229\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1230\u001B[39m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1237\u001B[39m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1238\u001B[39m ) -> ResponseT | _StreamT:\n\u001B[32m   1239\u001B[39m     opts = FinalRequestOptions.construct(\n\u001B[32m   1240\u001B[39m         method=\u001B[33m\"\u001B[39m\u001B[33mpost\u001B[39m\u001B[33m\"\u001B[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001B[32m   1241\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1242\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anna-apps/ml-and-engineering-docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:919\u001B[39m, in \u001B[36mSyncAPIClient.request\u001B[39m\u001B[34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[39m\n\u001B[32m    916\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    917\u001B[39m     retries_taken = \u001B[32m0\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m919\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    920\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    921\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    922\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    923\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    924\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    925\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anna-apps/ml-and-engineering-docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1023\u001B[39m, in \u001B[36mSyncAPIClient._request\u001B[39m\u001B[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001B[39m\n\u001B[32m   1020\u001B[39m         err.response.read()\n\u001B[32m   1022\u001B[39m     log.debug(\u001B[33m\"\u001B[39m\u001B[33mRe-raising status error\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1023\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._make_status_error_from_response(err.response) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1025\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._process_response(\n\u001B[32m   1026\u001B[39m     cast_to=cast_to,\n\u001B[32m   1027\u001B[39m     options=options,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1031\u001B[39m     retries_taken=retries_taken,\n\u001B[32m   1032\u001B[39m )\n",
      "\u001B[31mAuthenticationError\u001B[39m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************JZsA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}}"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# save load db if needed\n",
    "FAISS.load_local(\"index\", embeddings,allow_dangerous_deserialization=True)\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 4})"
   ],
   "id": "7a604cbf84d04c2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"You are a Nestle hiring assistant.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "Answer here:\"\"\"\n",
    "\n",
    "system_msg = (\n",
    "    \"You are a helpful HR assistant for Nestlé. \"\n",
    "    \"Answer ONLY using the provided context. \"\n",
    "    \"If the answer is not in the context, say you cannot find it in the policy. \"\n",
    "    \"Be concise and use bullet points when listing items. \"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_msg),\n",
    "        (\"human\", \"Question: {question}\\n\\nContext:\\n{context}\\n\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=CHAT_MODEL, temperature=0)\n",
    "parser = StrOutputParser()\n",
    "\n",
    "def _format_docs(docs):\n",
    "    return \"\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | _format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | parser\n",
    ")\n",
    "\n"
   ],
   "id": "9eed87ef7ef2a144"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Small helper to create page citations\n",
    "def pages_citation(docs):\n",
    "    pages = sorted({(d.metadata.get(\"page\", None) or 0) + 1 for d in docs})\n",
    "    if pages:\n",
    "        return \"Sources: \" + \", \".join(f\"p.{p}\" for p in pages)\n",
    "    return \"Sources: (none)\""
   ],
   "id": "eb57939e2c2423df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# -------- Gradio Chat Interface --------\n",
    "EXAMPLE_QUESTIONS = [\n",
    "    \"What is the leave policy (annual, sick, parental)?\",\n",
    "    \"How does the performance review process work?\",\n",
    "    \"Who do I contact for benefits enrollment?\",\n",
    "    \"What is the probation period for new employees?\"\n",
    "]\n",
    "\n",
    "# Chat function compatible with gr.ChatInterface\n",
    "def chat_fn(message, history):\n",
    "    try:\n",
    "        # Retrieve docs for explicit citation\n",
    "        docs = retriever.get_relevant_documents(message)\n",
    "        answer = rag_chain.invoke(message)\n",
    "        return answer + \"\\n\\n\" + pages_citation(docs)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chat_fn,\n",
    "    title=\"Nestlé HR Policy Assistant\",\n",
    "    description=(\n",
    "        \"Ask questions about Nestlé’s HR policy. \"\n",
    "        \"Answers are grounded in the uploaded PDF. \"\n",
    "        \"We'll append the page sources below each answer.\"\n",
    "    ),\n",
    "    examples=EXAMPLE_QUESTIONS,\n",
    "    textbox=gr.Textbox(placeholder=\"Type your HR question…\", container=True, scale=7)\n",
    ")\n"
   ],
   "id": "8a8f75cb2c51791e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "demo.launch()",
   "id": "8a47e9e89a61ca9c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "! pip install jupyter-nbconvert",
   "id": "c00a78d5a3e67681"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
