{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25837c28",
   "metadata": {},
   "source": [
    "# ðŸ§  AIâ€‘Powered HR Assistant â€” NestlÃ© HR Policy (Courseâ€‘End Project)\n",
    "\n",
    "This notebook demonstrates an endâ€‘toâ€‘end workflow to build a **conversational RAG chatbot** that answers queries about **NestlÃ©â€™s HR policy PDF** using:\n",
    "- **LangChain** (loading, splitting, retrieval pipeline)\n",
    "- **ChromaDB** (vector database)\n",
    "- **OpenAI** (embeddings + GPT chat model)\n",
    "- **Gradio** (chat UI)\n",
    "\n",
    "> **What you'll do:** set up the environment, load & chunk the PDF, create embeddings and a vector store, retrieve context, and use GPTâ€‘3.5â€‘Turbo to answer questions. Finally, you'll launch a Gradio chat app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4627a389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running locally, uncomment to install dependencies.\n",
    "# Note: You don't need to run this cell in the LMS if the environment already has them.\n",
    "# !pip -q install --upgrade pip\n",
    "# !pip -q install \"langchain>=0.2.6\" \"langchain-community>=0.2.6\" \"langchain-openai>=0.1.8\" \\    #                 \"chromadb>=0.5.3\" \"pypdf>=4.2.0\" \"tiktoken>=0.7.0\" \"gradio>=4.35.0\" \"python-dotenv>=1.0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d15916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "# Load .env if present (optional)\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# === IMPORTANT ===\n",
    "# Set your OpenAI API key securely (prefer .env file with OPENAI_API_KEY=...)\n",
    "# or set it in the environment before running the notebook.\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "assert os.getenv(\"OPENAI_API_KEY\"), \"Please set OPENAI_API_KEY in your environment or a .env file.\"\n",
    "\n",
    "# LangChain / OpenAI wrappers\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# LCEL building blocks\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# UI\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12fbc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Configuration ----------------\n",
    "PDF_PATH = \"data/nestle_hr_policy.pdf\"      # <-- Put your NestlÃ© HR policy PDF here\n",
    "PERSIST_DIR = \"chroma_db_nestle_hr\"         # persistent folder for Chroma\n",
    "EMBED_MODEL = \"text-embedding-3-small\"      # cost-effective; can use 'text-embedding-3-large' for higher quality\n",
    "CHAT_MODEL = \"gpt-3.5-turbo\"                # as per assignment; you may upgrade later (e.g., 'gpt-4o-mini')\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "print(f\"Expecting policy at: {PDF_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f6a183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Load PDF --------\n",
    "loader = PyPDFLoader(PDF_PATH)\n",
    "docs = loader.load()\n",
    "print(f\"Loaded {len(docs)} pages.\")\n",
    "\n",
    "# -------- Split into chunks --------\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=150, separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    ")\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "print(f\"Created {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3079b623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Create embeddings + VectorStore (Chroma) --------\n",
    "embeddings = OpenAIEmbeddings(model=EMBED_MODEL)\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=PERSIST_DIR,\n",
    ")\n",
    "vectordb.persist()\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 4})\n",
    "print(\"Vector store built & persisted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03699043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Prompt template for grounded answers --------\n",
    "system_msg = (\n",
    "    \"You are a helpful HR assistant for NestlÃ©. \"\n",
    "    \"Answer ONLY using the provided context. \"\n",
    "    \"If the answer is not in the context, say you cannot find it in the policy. \"\n",
    "    \"Be concise and use bullet points when listing items. \"\n",
    "    \"Do NOT fabricate citations; page references will be appended automatically.\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_msg),\n",
    "        (\"human\", \"Question: {question}\\n\\nContext:\\n{context}\\n\\nHelpful answer:\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=CHAT_MODEL, temperature=0)\n",
    "parser = StrOutputParser()\n",
    "\n",
    "def _format_docs(docs):\n",
    "    return \"\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "# LCEL RAG chain: {question} -> retrieve -> prompt -> LLM -> text\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | _format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | parser\n",
    ")\n",
    "\n",
    "# Small helper to create page citations\n",
    "def pages_citation(docs):\n",
    "    pages = sorted({(d.metadata.get(\"page\", None) or 0) + 1 for d in docs})\n",
    "    if pages:\n",
    "        return \"Sources: \" + \", \".join(f\"p.{p}\" for p in pages)\n",
    "    return \"Sources: (none)\"\n",
    "\n",
    "# Test a dry run (uncomment after you place the PDF):\n",
    "# q = \"What is the probation period policy for new hires?\"\n",
    "# docs_for_q = retriever.get_relevant_documents(q)\n",
    "# print(rag_chain.invoke(q) + \"\\n\\n\" + pages_citation(docs_for_q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa73cd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Gradio Chat Interface --------\n",
    "EXAMPLE_QUESTIONS = [\n",
    "    \"What are the working hours and overtime rules?\",\n",
    "    \"What is the leave policy (annual, sick, parental)?\",\n",
    "    \"How does the performance review process work?\",\n",
    "    \"Who do I contact for benefits enrollment?\",\n",
    "    \"What is the probation period for new employees?\",\n",
    "]\n",
    "\n",
    "# Chat function compatible with gr.ChatInterface\n",
    "def chat_fn(message, history):\n",
    "    try:\n",
    "        # Retrieve docs for explicit citation\n",
    "        docs = retriever.get_relevant_documents(message)\n",
    "        answer = rag_chain.invoke(message)\n",
    "        return answer + \"\\n\\n\" + pages_citation(docs)\n",
    "    except Exception as e:\n",
    "        return f\"âš ï¸ Error: {e}\"\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chat_fn,\n",
    "    title=\"NestlÃ© HR Policy Assistant\",\n",
    "    description=(\n",
    "        \"Ask questions about NestlÃ©â€™s HR policy. \"\n",
    "        \"Answers are grounded in the uploaded PDF. \"\n",
    "        \"We'll append the page sources below each answer.\"\n",
    "    ),\n",
    "    examples=EXAMPLE_QUESTIONS,\n",
    "    textbox=gr.Textbox(placeholder=\"Type your HR questionâ€¦\", container=True, scale=7),\n",
    "    retry_btn=\"Retry\",\n",
    "    undo_btn=None,\n",
    "    clear_btn=\"Clear\",\n",
    ")\n",
    "\n",
    "# To run locally: uncomment the next line.\n",
    "# demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc78478",
   "metadata": {},
   "source": [
    "## ðŸ”Ž (Bonus) Query Structuring\n",
    "In some cases, you may wish to **rewrite** the user's question into a clearer, more retrievalâ€‘friendly form.\n",
    "The snippet below shows how to build a lightweight \"query rewriter\" using the same chat model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc4d1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "rewrite_template = PromptTemplate.from_template(\n",
    "    \"Rewrite the user question into a short, clear search query for HR policy retrieval.\\n\"\n",
    "    \"User question: {q}\\n\"\n",
    "    \"Rewritten search query:\"\n",
    ")\n",
    "\n",
    "def rewrite_query(q: str) -> str:\n",
    "    rewritten = (rewrite_template | llm | parser).invoke({\"q\": q}).strip()\n",
    "    return rewritten or q\n",
    "\n",
    "# Example:\n",
    "# rewrite_query(\"Could you explain the rules for taking vacation and sick leave?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d808249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_hr(question: str, use_rewriter: bool = False) -> str:\n",
    "    q = rewrite_query(question) if use_rewriter else question\n",
    "    docs = retriever.get_relevant_documents(q)\n",
    "    answer = rag_chain.invoke(q)\n",
    "    return answer + \"\\n\\n\" + pages_citation(docs)\n",
    "\n",
    "# Example:\n",
    "# ask_hr(\"What are the maternity leave benefits?\", use_rewriter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721091f6",
   "metadata": {},
   "source": [
    "## âœ… Result\n",
    "You now have a working **RAG chatbot** for NestlÃ©â€™s HR policy with:\n",
    "- PDF ingestion & chunking\n",
    "- Embeddings & vector search (Chroma)\n",
    "- GPTâ€‘3.5â€‘Turbo question answering grounded in retrieved context\n",
    "- A Gradio chat interface with page citations\n",
    "\n",
    "### Submission tips\n",
    "- Ensure your **`data/nestle_hr_policy.pdf`** file exists before running.\n",
    "- Run all cells endâ€‘toâ€‘end and include relevant screenshots in your LMS submission.\n",
    "- You can export the notebook to HTML (`File â†’ Save and Export Notebook Asâ€¦ â†’ HTML`) if required."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
